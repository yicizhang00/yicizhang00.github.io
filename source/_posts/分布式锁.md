---
title: 分布式锁
date: 2025-09-22 03:22:40
tags:
  - 分布式
---

# 分布式理论/协议/算法基础

## CAP理论

**CAP** 也就是 **Consistency（一致性）**、**Availability（可用性）**、**Partition Tolerance（分区容错性）** 这三个单词首字母组合。

**一致性（Consistency）** : 所有节点访问同一份最新的数据副本

**可用性（Availability）**: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。

**分区容错性（Partition Tolerance）** : 分布式系统出现网络分区的时候，仍然能够对外提供服务。

>• consistency (C) equivalent to having a single up-to-date copy of the data;
>• high availability (A) of that data (for updates); and
>• tolerance to network partitions (P).

CAP理论并不是3选2的选择题，而是AP和CP中选择：

**当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。**

简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。

CAP实际上只禁止了一种极端情况：当发生分区时，系统不可能保证一致性和可用性，但是实际上网络分区在真实网络中相对少见，大多数情况下网络是能够满足强一致性和可用性的。

理解 CAP 最简单的方法是：想象两个节点被网络分区隔开在两边。
 如果允许至少一个节点继续更新状态，那么节点之间就会出现不一致，从而放弃了一致性（C）。
 相反，如果选择保持一致性，那么分区的一侧必须表现得像是不可用，从而放弃了可用性（A）。
 只有在节点之间能够通信时，才能同时保持一致性和可用性，而这意味着放弃了分区容忍性（P）。
 普遍的看法是，对于广域系统（wide-area systems），设计者无法放弃 P，因此只能在一致性（C）和可用性（A）之间做出艰难的选择。
 从某种意义上说，NoSQL 设计的核心就是在设计时优先考虑可用性，其次才是一致性；而遵循 ACID 特性的数据库（原子性、一致性、隔离性、持久性）则恰好相反。





## ACID/BASE和CAP

ACID的设计是单机型数据库的设计原则，为了保证数据的强一致性，而BASE理论则是分布式系统/数据库设计原则，为了保证数据的高可用和高扩展，允许数据的短暂不一致，只需要保证最终一致性即可。

### ACID

ACID 是数据库事务的四个关键特性，保证了数据操作的 **可靠性**。
 它分别是：**原子性 (Atomicity)、一致性 (Consistency)、隔离性 (Isolation)、持久性 (Durability)**。

1. 原子性：事务中的所有操作要么全部成功要么全部不成功，通常采用Undo Log实现，来回滚失败的事务。
2. 一致性：事务执行前后，数据库必须从一个 **一致状态** 转换到另一个 **一致状态**。所有数据必须满足数据库定义的约束（如主键唯一、外键约束、触发器、检查约束等）。假设数据库要求账户余额不能为负数，那么事务执行结束后，必须仍然满足这个规则。
3. 隔离性：多个事务同时执行，每个事务应该是隔离的，数据库有不同的隔离等级。

- 读未提交
- 读已提交->解决脏读
- 可重复读->解决不可重复度
- 串行化->解决幻读

4. 持久性：一旦事务提交成功，它对数据库的修改就是永久性的，即使系统崩溃也不会丢失。实现方式：数据库通常通过 **WAL（Write Ahead Log，预写日志）** 或 **Redo Log** 来保证。

### ACID和CAP的对比

**原子性 (Atomicity, A)**
 所有系统都能从原子操作中获益。即使系统重点在于可用性，分区两边也应该继续使用原子操作。此外，更高层级的原子操作（ACID 所要求的那种）实际上能简化恢复过程。

**一致性 (Consistency, C)**
 在 ACID 中，C 的含义是事务必须保持数据库的所有规则，例如唯一键约束。相比之下，CAP 中的 C 仅指 **单副本一致性**，这是 ACID 一致性的严格子集。ACID 的一致性无法跨分区维持——分区恢复时必须重新建立 ACID 一致性。更一般地说，在分区期间保持数据库不变式（invariants）可能是不可能的，因此必须仔细考虑：哪些操作需要禁止，以及在恢复时如何恢复这些不变式。

**隔离性 (Isolation, I)**
 隔离性是 CAP 定理的核心：如果系统要求 ACID 的隔离性，在分区期间它最多只能在一侧运行。**可串行化（serializability）** 一般需要通信，因此无法跨分区维持。较弱的正确性定义在分区期间是可行的，可以通过分区恢复时的补偿机制来实现。

**持久性 (Durability, D)**
 和原子性一样，没有理由放弃持久性，尽管开发者可能会为了降低代价而选择不依赖持久性，而是采用类似 BASE 的“软状态”。
 一个微妙的问题是，在分区恢复过程中，可能需要撤销某些持久化操作，因为它们在执行时无意间违反了某个不变式。不过，在恢复时，由于两边都保留了持久化的历史记录，因此可以检测并修正这些操作。总体而言，在分区两侧都运行 ACID 事务可以使恢复更容易，并为补偿性事务（compensating transactions）提供框架，用于分区恢复。



### BASE理论

BASE 理论的核心思想是：**放弃强一致性，接受数据的短暂不一致，从而确保系统的高可用性和分区容错性**。

### BASE 的含义

BASE 是一个缩写，它代表：

1. **Basically Available (基本可用)**
   - **含义**：分布式系统在出现不可预知的故障（如网络分区、节点宕机）时，系统保证**核心功能**依然可用，允许损失部分功能或体验（如响应时间变长、服务降级）。
   - **例子**：双十一期间，为了应对海量请求，淘宝可能会关闭“查看历史订单详情”等非核心功能，或者将用户引导至一个降级页面，但核心的“下单、支付”功能必须保持可用。这牺牲了部分可用性，保证了核心可用，即“基本可用”。
2. **Soft State (软状态)**
   - **含义**：允许系统中的数据存在**中间状态**，并且这个状态即使没有新的输入，也可能随着时间而变化。这个状态是“软”的，因为它不需要在每一刻都完全准确，允许不同节点的数据副本之间存在暂时的不一致。
   - **例子**：在微博中，你发布一条消息后，你的粉丝可能不会立刻看到。系统可能会需要几秒钟甚至几分钟的时间，将这条新微博异步地复制到所有粉丝的“信息流”缓存中。在这段复制时间内，数据就处于一种“软状态”——它确实存在，但并非所有用户都能立即访问到。
3. **Eventual Consistency (最终一致性)**
   - **含义**：这是 BASE 理论的最终目标。它保证如果系统在一段时间内没有收到新的数据更新操作，那么最终所有对数据的访问请求都会返回**最新写入的值**。也就是说，系统保证数据副本最终会达成一致，但不保证立即一致。
   - **例子**：继续上面的微博例子。系统不保证你的粉丝A和粉丝B在同一秒看到你的新微博，但它保证，只要没有新的更新，在几秒或几分钟后，所有粉丝刷新他们的信息流时，最终都会看到这条微博。

### 一个经典的例子：电商库存

- **ACID 方式**：
  - 用户下单扣减库存时，数据库会立即锁定这条库存记录（**隔离性**），完成扣减和订单创建后（**原子性**）才释放锁。在此期间，其他所有用户看到的库存数都是扣减后的**一致**结果。这保证了不会超卖，但高并发时，锁竞争会导致性能瓶颈和响应延迟（可用性降低）。
- **BASE 方式**：
  - 用户下单时，系统先扣减**缓存中的库存**，并快速返回成功，允许用户下单（**基本可用**）。此时，缓存中的库存和数据库中的实际库存是**不一致**的（**软状态**）。
  - 系统通过一个后台任务，异步地将库存扣减操作**最终**同步到主数据库中（**最终一致性**）。
  - 这种方式吞吐量极高，用户体验好。但极端情况下（比如库存只剩1件，瞬间有大量请求），可能会发生“超卖”（数据短暂不一致的体现），这时系统通常通过后续的补救措施（如道歉、补偿）来处理。

### 总结

**BASE 理论不是 ACID 的替代，而是一种补充和权衡**。它是构建大型分布式系统时的一种设计哲学，其诞生源于对**可用性**和**分区容错性**的追求，并坦然接受了由此带来的**最终一致性**后果。

## 超时

在1999年提出的经典CAP理论中，忽略了超时对分区的影响，实际上**延迟与分区是紧密相关的**。
 在运维层面，CAP 的核心体现于 **超时** 的那一刻：程序必须做出一个根本性的决定——即 **分区决策**：

- **取消操作**，从而降低可用性，或
- **继续执行操作**，但冒着出现不一致的风险。

为了保证一致性而进行通信重试（例如使用 **Paxos** 或 **两阶段提交**），其实只是延迟了这个决策。
 最终，程序必须在某个时刻做出选择；而如果无限期地重试通信，本质上就是在 **一致性 (C)** 和 **可用性 (A)** 之间选择了 **一致性 (C)**。

其实分区就是超出了超时的网络，PAXOS协议会选择无限等待，这意味着PAXOS实际上选了CP而不是AP。

### 一致性

**一致性的范围** 指的是：在某个边界之内，系统状态是一致的，而超出该边界，就无法保证。例如，在一个主分区（primary partition）内部，可以同时保证一致性和可用性；但在分区之外，服务就不可用了。Paxos 和 **原子多播（atomic multicast）** 系统通常就是这种情况。以 Google 为例，主分区通常位于一个数据中心内部；而在广域范围内，则使用 Paxos 来确保全局共识（如 Chubby），以及高可用的持久化存储（如 Megastore）。

CAP的一致性是有边界的，例如RAFT协议中，访问没有来得及跟上主节点的从节点，这部分数据就是会不一致。



## 分支管理

在强一致性(C)的系统中时，我们选择放弃可用性（A），此时系统会自动保证事务或数据更新不会破坏一些**不变式 (invariants)**。这些不变式是强一致性保证的，

- 如果选择 **高可用 (A)**（比如 BASE 模型中的 *Eventually Consistent* 系统），在网络分区时不同节点可能接受到不同的更新，导致系统短时间内违反不变式。
- 当网络恢复后，就必须 **恢复一致性**，这时需要显式知道：
  - 哪些不变式被破坏了？
  - 如何恢复？
  - 需要做什么补偿操作？

👉 这就要求设计者必须 **明确地列出并编码所有不变式**，否则恢复阶段没法保证数据正确。

当我们选择AP的时候，很容易出现分支，如下图

![image-20250924025901203](https://raw.githubusercontent.com/yicizhang00/image_host/main/blog-img/202509240259367.png)

State从$S$演变出两个分支$S_1$和$S_2$，在解决网络分区后我们需要知道哪些分支是可以合并的。

分区模式后，有两种可能的策略：第一种是限制某些操作，从而降低可用性；第二种是记录关于操作的额外信息，这些信息将在分区恢复期间发挥作用。持续尝试通信将使系统能够辨别分区何时结束。

在分区发生时，我们要设计哪些操作是分区可以容忍的，而哪些操作不行，分区结束后，我们要对分区进行合并，以及补偿所产生的错误。

INRIA 的 Marc Shapiro 及其同事们的最新研究显著改进了利用**可交换操作**（commutative operations）来实现状态收敛的方法。团队提出了**可交换复制数据类型（CRDTs）**，这是一类能够在网络分区后**可证明地收敛**的数据结构，并描述了如何使用这些结构来：

- 确保在分区期间的所有操作都是**可交换的**，或者
- 将值表示在一个**格结构（lattice）\**上，并确保在分区期间的所有操作在该格上都是\**单调递增**的。

后一种方法通过取每一方值的**最大值**来实现状态收敛。这是对亚马逊购物车做法的形式化和改进：在分区之后，最终收敛的结果是两个购物车的**并集**，而并集是一种单调的集合操作。这样做的后果是，被删除的商品可能会重新出现。

然而，CRDT 也能实现同时支持**添加**和**删除**操作的**分区容忍集合**。其核心思想是维护两个集合：一个记录新增项，另一个记录删除项，最终的集合成员由两者的差集决定。每个简化后的集合都会收敛，因此它们的差集也会收敛。在某个时间点，系统可以通过将已删除的元素从两个集合中移除来进行清理。不过，这种清理通常只能在系统不发生分区时进行。换句话说，设计者必须在分区期间**禁止或延迟部分操作**，但这些仅仅是清理操作，不会影响用户感知的可用性。

因此，通过使用 CRDT 来实现状态，设计者可以在选择 **高可用性 (A)** 的同时，仍然确保系统在分区后能**自动收敛**。

## Zookeeper

Zookeeper保证的是**CP**，使用**ZAB协议**。任何时刻对Zookeeper的请求都会保证一致性，假设一种情况有Zookeeper集群有5个节点，发生网络分区，分为3个和2个，两个子集不能相通信。此时：

- 3 个节点组成 **多数派** → 继续提供服务

- 2 个节点组成 **少数派** → 不提供服务

如果有节点连接到了多数派，Zookeeper节点正常提供服务，如果有其他服务连到了少数派，则Zookeeper节点会挂起。

## Eureka

**Eureka 保证的则是 AP。** Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么 Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper 那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka 保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。



## Nacos

nacos不仅支持AP也支持CP，其作为配置中心的功能主要实现的是CP，强调的是强一致性。作为注册中心i的功能实现的是AP，强调的是可用性。



## PAXOS协议

paxos协议的论文原文过于晦涩难懂



## RAFT协议



## KRAFT协议





## ZAB协议

